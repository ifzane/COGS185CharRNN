{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Create a function to calculate the sum of a se...</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td># Python code\\ndef sum_sequence(sequence):\\n  ...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Generate a Python code for crawling a website ...</td>\n",
       "      <td>website: www.example.com \\ndata to crawl: phon...</td>\n",
       "      <td>import requests\\nimport re\\n\\ndef crawl_websit...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Create a Python list comprehension to get the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[x*x for x in [1, 2, 3, 5, 8, 13]]</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Generate a python script to perform this action.</td>\n",
       "      <td>Given a string, remove all the consecutive dup...</td>\n",
       "      <td>def remove_duplicates(string): \\n    result = ...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Write a python script to generates random numb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>def generate_random_divisible_number():\\n    i...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  Create a function to calculate the sum of a se...   \n",
       "1  Generate a Python code for crawling a website ...   \n",
       "2  Create a Python list comprehension to get the ...   \n",
       "3   Generate a python script to perform this action.   \n",
       "4  Write a python script to generates random numb...   \n",
       "\n",
       "                                               input  \\\n",
       "0                                    [1, 2, 3, 4, 5]   \n",
       "1  website: www.example.com \\ndata to crawl: phon...   \n",
       "2                                                NaN   \n",
       "3  Given a string, remove all the consecutive dup...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                              output  \\\n",
       "0  # Python code\\ndef sum_sequence(sequence):\\n  ...   \n",
       "1  import requests\\nimport re\\n\\ndef crawl_websit...   \n",
       "2                 [x*x for x in [1, 2, 3, 5, 8, 13]]   \n",
       "3  def remove_duplicates(string): \\n    result = ...   \n",
       "4  def generate_random_divisible_number():\\n    i...   \n",
       "\n",
       "                                              prompt  \n",
       "0  Below is an instruction that describes a task....  \n",
       "1  Below is an instruction that describes a task....  \n",
       "2  Below is an instruction that describes a task....  \n",
       "3  Below is an instruction that describes a task....  \n",
       "4  Below is an instruction that describes a task....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Python code\n",
      "def sum_sequence(sequence):\n",
      "  sum = 0\n",
      "  for num in sequence:\n",
      "    sum += num\n",
      "  return sum\n"
     ]
    }
   ],
   "source": [
    "print(df['output'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'w', encoding='utf-8') as file:  # Specify UTF-8 encoding\n",
    "    # Iterate through the entries in the \"output\" column\n",
    "    for entry in df['output']:\n",
    "        # Write the entry to the file\n",
    "        file.write(entry.strip())  # Remove leading/trailing whitespaces\n",
    "        file.write('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "all_chars = string.printable\n",
    "n_chars = len(all_chars)\n",
    "\n",
    "with open('./output.txt', 'r', encoding='utf-8') as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "file_len = len(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of file: 8727916\n",
      "All possible characters: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b\f\n",
      "Number of all possible characters: 100\n"
     ]
    }
   ],
   "source": [
    "print('Length of file: {}'.format(file_len))\n",
    "print('All possible characters: {}'.format(all_chars))\n",
    "print('Number of all possible characters: {}'.format(n_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_seq(seq_len=50):\n",
    "    start_index = random.randint(0, file_len - seq_len - 1)\n",
    "    end_index = start_index + seq_len\n",
    "    return file_content[start_index:end_index]\n",
    "\n",
    "def get_input_and_target(batch_size):\n",
    "    seq = get_random_seq()\n",
    "    input_seq = seq[:-1]\n",
    "    target_seq = seq[1:]\n",
    "    input_tensor = seq_to_onehot(input_seq)\n",
    "    target_tensor = seq_to_index(target_seq)\n",
    "\n",
    "    input_tensor = input_tensor.repeat(batch_size, 1, 1)\n",
    "    target_tensor = target_tensor.repeat(batch_size).view(-1)  # Flatten the target tensor\n",
    "\n",
    "    return input_tensor, target_tensor\n",
    "\n",
    "def seq_to_onehot(seq):\n",
    "    tensor = torch.zeros(len(seq), n_chars)\n",
    "    for t, char in enumerate(seq):\n",
    "        try:\n",
    "            index = all_chars.index(char)\n",
    "            tensor[t][index] = 1\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return tensor.unsqueeze(0)  # Add batch dimension.\n",
    "\n",
    "def seq_to_index(seq, special_index=-1):\n",
    "    tensor = torch.zeros(len(seq), dtype=torch.long)\n",
    "    for t, char in enumerate(seq):\n",
    "        try:\n",
    "            index = all_chars.index(char)\n",
    "            tensor[t] = index\n",
    "        except ValueError:\n",
    "            tensor[t] = special_index\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  \n",
    "# If 'cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(YourRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])  # Use only the last output\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.lstm(input, hidden)\n",
    "        output = self.fc(output)  # Apply the fully connected    layer to each time step\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(1, batch_size, self.hidden_size).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(net, opt, input, target):\n",
    "    opt.zero_grad()\n",
    "    hidden = net.init_hidden(input.size(0))  # Initialize with actual batch size\n",
    "    output, hidden = net(input, hidden)\n",
    "    output = output.view(-1, n_chars)  # Flatten output to match target shape\n",
    "    loss = loss_func(output, target)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(net, init_seq='W', predicted_len=100):\n",
    "    batch_size = 1\n",
    "    hidden = net.init_hidden(batch_size)\n",
    "    init_input = seq_to_onehot(init_seq).to(device)\n",
    "    predicted_seq = init_seq\n",
    "\n",
    "    # Use the initial sequence to prime the hidden state\n",
    "    for t in range(len(init_seq) - 1):\n",
    "        output, hidden = net(init_input[:, t:t+1, :], hidden)\n",
    "        \n",
    "    # Start generating new characters\n",
    "    input = init_input[:, -1:, :]  # Use the last character of the initial sequence\n",
    "    input = input.to(device)\n",
    "    \n",
    "    for t in range(predicted_len):\n",
    "\n",
    "        output, hidden = net(input, hidden)\n",
    "        predicted_index = torch.multinomial(output[:, -1, :].exp(), 1)[0]\n",
    "        predicted_char = all_chars[predicted_index]\n",
    "        predicted_seq += predicted_char\n",
    "        \n",
    "        # Prepare the next input\n",
    "        input = seq_to_onehot(predicted_char).to(device)  # Adjust input shape\n",
    "\n",
    "    return predicted_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:999/100000 loss:2.919465290427208\n",
      "Generated sequence: WYaw+elentlfnnt2\n",
      "\n",
      "#adeans cont.re,\n",
      "==\"Lhom, gals)):\n",
      " dodet(rumT, ren(tet(eys-]\n",
      "pment( ls))\n",
      "\n",
      "     \n",
      "#f.\n",
      "\n",
      "iter:1999/100000 loss:2.3494391055703163\n",
      "Generated sequence: Wmue tard.ifpr== coloate\"\n",
      "    \n",
      "        arls = cost__strong/-tring_vex(M-Ficeac.in, ingre(print(10\n",
      "\n",
      "pr\n",
      "\n",
      "iter:2999/100000 loss:2.1553306364417075\n",
      "Generated sequence: Weth tatpagagtimo\" yze ra:allete_numsation.mpviy the ort the mocsse\n",
      "\n",
      "in repet\n",
      "3\n",
      "           return ]e \n",
      "\n",
      "iter:3999/100000 loss:1.9999844109416007\n",
      "Generated sequence: Word = ms-S%-jons = msconigre)\n",
      "    nemale__ustrince()\n",
      "\n",
      "\n",
      "import stlf = apuest_tixct = 8 Ressing The mo\n",
      "\n",
      "iter:4999/100000 loss:1.9468916385173798\n",
      "Generated sequence: Word'!is as + port\n",
      "\\targed aur node tha <Nomest!\n",
      "    if lit(qlte ss6vepterstReLust'%sQlistroment'] da\n",
      "\n",
      "iter:5999/100000 loss:1.8776803296804427\n",
      "Generated sequence: WerCods') \n",
      "    return enlime = statess[is['\\n-1)'\n",
      "\n",
      "def bustlor_nott_mel__hever[i] = datg_arr/porker:\n",
      "\n",
      "\n",
      "iter:6999/100000 loss:1.8092242192141712\n",
      "Generated sequence: W8:2]\n",
      "\n",
      "\tbf.TSU)\n",
      "\n",
      "# defuner pot the grageinem\n",
      "Modelf = 10.540/2-*i*0, 3, 2)\n",
      "        print(kytal creat \n",
      "\n",
      "iter:7999/100000 loss:1.7722935312390327\n",
      "Generated sequence: Wettor_number)\n",
      "\t\t\tdemSying2, yy=apparse\" \n",
      "    renden $pdist = dato.append(i>^-1]\n",
      "\n",
      "def _siteThalbe_siz\n",
      "\n",
      "iter:8999/100000 loss:1.752531723089516\n",
      "Generated sequence: W\", 1))\n",
      "check_oright = [{\n",
      " Jodd value left popltodet root exent te trint[i]\n",
      "            return field \n",
      "\n",
      "iter:9999/100000 loss:1.7190868884921073\n",
      "Generated sequence: WGr Funcorator expression for my impolitiel convert for, nlosuling2.crant, secceax.classing)\n",
      "popes_wi\n",
      "\n",
      "iter:10999/100000 loss:1.7125680844262243\n",
      "Generated sequence: WIYONUSF-S=crast_n):\n",
      "            string = tokenize_j+1]:\n",
      "  \n",
      "# sort array maxpse\n",
      "\n",
      "#!9\\NTimeSerge.ints:\n",
      "\n",
      "iter:11999/100000 loss:1.6621275459527969\n",
      "Generated sequence: Werd_data('dataingent:\", text_start=ince, resplt)\n",
      "\n",
      "\n",
      "llint.predictions = []\n",
      "    don_val = ne.response.\n",
      "\n",
      "iter:12999/100000 loss:1.6487985949777066\n",
      "Generated sequence: Wandon_dfand: <= 50 \n",
      "print(userqessivate(wrepralges)], andor, weithon == 'ialure')\n",
      "\n",
      "Train the sum_lyp\n",
      "\n",
      "iter:13999/100000 loss:1.69779568329826\n",
      "Generated sequence: Wlayud_poscess +mportions.calculate(sel,=pry.tan(x)\n",
      "\n",
      "ltk_list.median(\"Arl\", \")\n",
      "\n",
      "# This type as giod o\n",
      "\n",
      "iter:14999/100000 loss:1.6147287137210369\n",
      "Generated sequence: WRETHOUTP':\n",
      "    or i in list of and\n",
      "\n",
      "xew__Buting -= sentiment_empy_surinul(files):\n",
      "                se\n",
      "\n",
      "iter:15999/100000 loss:1.6632217268049716\n",
      "Generated sequence: Weeght 'fror a !> -1:\n",
      "\tmade_words(rid = pabe - string['hict': 'OdF'']\n",
      "\n",
      "sumConNen = size(seb1)\n",
      "\n",
      " retur\n",
      "\n",
      "iter:16999/100000 loss:1.6066200279891492\n",
      "Generated sequence: Wersoms \\w}\n",
      "\n",
      "secrete = node.recend(project)\n",
      "X, htpte_file=(x.append())\n",
      "#\n",
      "zX_POTRO\n",
      "    djangoxExtors\n",
      "f\n",
      "\n",
      "iter:17999/100000 loss:1.5998619954958557\n",
      "Generated sequence: Work input lengthaccore tinp inputher to toot\",notte\n",
      "\n",
      "\n",
      "import name\n",
      " return self.name\n",
      "        return F\n",
      "\n",
      "iter:18999/100000 loss:1.632586873114109\n",
      "Generated sequence: Word require Bee\n",
      " def platasVhash_nembwier(con_wind +  5): \n",
      "                        '\"text: ance] = [\n",
      "\n",
      "iter:19999/100000 loss:1.5907129860818385\n",
      "Generated sequence: Weddd_None,\n",
      "             data\n",
      "            age = textrowCreatity to pow self.values of the po the al\n",
      "m\n",
      "\n",
      "iter:20999/100000 loss:1.5808194338679313\n",
      "Generated sequence: WH = of_convext.maxNegrandlensTrieh(\n",
      "            = {} \n",
      "        prediction, [lentl_medays) + 'chouge}'\n",
      "\n",
      "iter:21999/100000 loss:1.5958898515701294\n",
      "Generated sequence: WbValual to\n",
      "s bOl Node b# \": {>> ware ds.Cra\"\n",
      "\n",
      "\n",
      "classs = vPHESICT0\n",
      "        178, 'Em_,'inter\"\n",
      "        \n",
      "\n",
      "iter:22999/100000 loss:1.5632587736397983\n",
      "Generated sequence: W) = 2\n",
      "mode = req.move(\"atter\"] for purnncoms vervicolNether.userics: l + spletp._array(self, conseti\n",
      "\n",
      "iter:23999/100000 loss:1.5192109596133232\n",
      "Generated sequence: WY S_num_inversicates\n",
      "\n",
      "print_jockaild(word) \n",
      "  for num in result:\n",
      "       \n",
      "sum = [-1:\n",
      "                \n",
      "\n",
      "iter:24999/100000 loss:1.5496130212545396\n",
      "Generated sequence: Word the pep to exploog\n",
      "from flag dictionary: \n",
      "                return class:\n",
      "    prefils=None.adgNate\n",
      "\n",
      "iter:25999/100000 loss:1.544504050642252\n",
      "Generated sequence: Words data\n",
      "def _initt_podle_list.append(parceparecinnambtryprime=cattore):\n",
      "        prime_list = \"DuxE\n",
      "\n",
      "iter:26999/100000 loss:1.558410443380475\n",
      "Generated sequence: Worddul code x Oat'\n",
      "CSEST = vendorTest.get * \\\n",
      "\n",
      "\t\t\tif self. .. endion\n",
      "\n",
      "grapezer.positics():\n",
      " result =\n",
      "\n",
      "iter:27999/100000 loss:1.5829221505820752\n",
      "Generated sequence: Wref each Swordtedsory\n",
      "pred = False) \n",
      "\n",
      "def exceps=Back.jsan(x, end):\n",
      "    sqltems.Cares = sorted(URL.s\n",
      "\n",
      "iter:28999/100000 loss:1.5683021229356526\n",
      "Generated sequence: WPAR), 'RmX trainer is\"\"\"\n",
      "\n",
      "\n",
      "# Prices intent[key]: \n",
      "           end.textIntex / de.otor(msv'))\n",
      "print( X\n",
      "\n",
      "iter:29999/100000 loss:1.5688867853879929\n",
      "Generated sequence: WIST. Randv [1\n",
      "                # Get the words and on train custional precess \n",
      "   project_list = comm\n",
      "\n",
      "iter:30999/100000 loss:1.540059747260064\n",
      "Generated sequence: Wr=operatwork to has af inputs\n",
      "    def __init__(self):\n",
      "    result = oper_scored_y(lest, self):\n",
      "  retu\n",
      "\n",
      "iter:31999/100000 loss:1.5478715144991875\n",
      "Generated sequence: WRORO STC(')',\n",
      " f            BRTH % 7)\n",
      "   return i\", lobel(zerl===0))\n",
      "        '[item/bags'] / 22+1 % \n",
      "\n",
      "iter:32999/100000 loss:1.5140879533886908\n",
      "Generated sequence: With pytange\n",
      "def caviv_res = 5\n",
      "\n",
      "sort()\n",
      "\n",
      "# Stam(\"^{\" 1}\": \"stuten|\"+1.20, request.age.data(isf, fix)\n",
      " \n",
      "\n",
      "iter:33999/100000 loss:1.5330537056922913\n",
      "Generated sequence: WRTIONTSTPRAPIUNAREA:  O Wuile found Get the emit caderate_results\n",
      "\n",
      " neadar = find_list[j + 1] == els\n",
      "\n",
      "iter:34999/100000 loss:1.5289553894400596\n",
      "Generated sequence: Word_newe(lizicator)\n",
      "  except (sprices, common_data) from sklearner:\n",
      "        cmave_counter\n",
      "print([\"Pa\n",
      "\n",
      "iter:35999/100000 loss:1.5050228158533574\n",
      "Generated sequence: Weasu {np.turns sorted \"\"\"\n",
      "\n",
      "        while n in longiter:\n",
      "            print(find_else)\n",
      "  \n",
      "        midr\n",
      "\n",
      "iter:36999/100000 loss:1.5286213592588902\n",
      "Generated sequence: WUVScaless\n",
      "def cest_innencl(n): \n",
      "       \n",
      "        return False\n",
      "        return COS The Lame for by uppe\n",
      "\n",
      "iter:37999/100000 loss:1.5126893449425698\n",
      "Generated sequence: Wh')\n",
      "\n",
      "# Mynumber a ds prime type=createx\n",
      "           yx = (\"unit\")\n",
      "        slock=y[11] = datetorizer.s\n",
      "\n",
      "iter:38999/100000 loss:1.5091587601751089\n",
      "Generated sequence: WID WARPL; WHERE |\n",
      "  if GE_NcielationA.grource:))\n",
      "for len rasing -j - - \")\n",
      "print(f'{\n",
      "    # O=k for it\n",
      "\n",
      "iter:39999/100000 loss:1.4926010776311158\n",
      "Generated sequence: W[ 1] + (int]\n",
      "Yrayved_valuesDistem[i, \"/'sdj1, corInter set\": \"Decrended')\n",
      "    \"\"\"\n",
      "        self.__rat\n",
      "\n",
      "iter:40999/100000 loss:1.514145191565156\n",
      "Generated sequence: WD_INT\n",
      "        for i in lise \n",
      "  return input(\"Window g new \"capoder\"\n",
      "\n",
      "# learger\n",
      "longest = \"squarl\n",
      "sou\n",
      "\n",
      "iter:41999/100000 loss:1.5124351833947003\n",
      "Generated sequence: WLFIVE_rving the bOQCITION.find()\n",
      "import random\n",
      "\n",
      "def urlMate.apfaldS.fit(X, y)\n",
      "\n",
      "\n",
      "def event_nodes(\")\n",
      " \n",
      "\n",
      "iter:42999/100000 loss:1.502781541466713\n",
      "Generated sequence: Wr(y = numpy / mnt(input(\"Record\":\n",
      "        data[item][nun+str.primez='pridity').source ('writt')[c32]\n",
      "\n",
      "iter:43999/100000 loss:1.4858124915584923\n",
      "Generated sequence: WantClassy\", \"Whb< \"Task: Set in the data file pays: later daye specter chole indext: \" \", duenu)\n",
      "\n",
      "# \n",
      "\n",
      "iter:44999/100000 loss:1.4746060650795698\n",
      "Generated sequence: WT_ETS), 1):\n",
      "             >  is_A):\n",
      "          c = 'husihale\": (capi2.db[s2-1]).pinl(self.key=cnigeS2)\n",
      "\n",
      "iter:45999/100000 loss:1.5072981953918934\n",
      "Generated sequence: WL\n",
      "right_scalingutions += 1\n",
      "            print('The addrating given python a di6c worder it %dof')\n",
      "\n",
      "\n",
      "s\n",
      "\n",
      "iter:46999/100000 loss:1.4825600425302983\n",
      "Generated sequence: Wh: alter(lst[minute_for_key):\n",
      " if __name__ == n__NO ta: -us>\n",
      "def ol_stackLicksformMain('Price fina h\n",
      "\n",
      "iter:47999/100000 loss:1.4913578716516496\n",
      "Generated sequence: WACKCTT = 1\n",
      "    def __init__(self, baL4)):\n",
      "  result.append(w*y(dinBche(k[\"-price\"]))\n",
      "print(generate_c\n",
      "\n",
      "iter:48999/100000 loss:1.4810814286768437\n",
      "Generated sequence: WPQURERETT\", 'e':'', 3 1\n",
      "        return \"2:14565\n",
      "}.........w...out_gcarmeDrig.Bne'\n",
      "    self.sonked = \n",
      "\n",
      "iter:49999/100000 loss:1.489668599128723\n",
      "Generated sequence: WHANSES\n",
      "      # Spandatanygop sorted of the #i pandas a frequest words, ad ape, vtring, argity..-lf.k\n",
      "\n",
      "iter:50999/100000 loss:1.4654310826919974\n",
      "Generated sequence: Widthon from the recodirged\"\n",
      " regals = [4, 9]]\n",
      "from <ymin_frequency:\n",
      " def csv', producesslume=[:].jso\n",
      "\n",
      "iter:51999/100000 loss:1.4709198909699917\n",
      "Generated sequence: Wipp BIvel \", [1, 1, 2, 1, 5, 3, 5, 0]\n",
      "= \t\t\t\tchiloring = to_out_fored.seefiam()\n",
      "        t, (250))\n",
      "pri\n",
      "\n",
      "iter:52999/100000 loss:1.4656493244506419\n",
      "Generated sequence: W[i-1] - '0(leat_port-backtepent-mdC//bx') # # Input\"\n",
      "    return self.fig\n",
      "This ____nene indedocced_wo\n",
      "\n",
      "iter:53999/100000 loss:1.4663939028009771\n",
      "Generated sequence: World emeshondfrimberm.month.pyneced:\n",
      "        det = onerir((4), c()\n",
      "\n",
      "        for c in random nvmar_ra\n",
      "\n",
      "iter:54999/100000 loss:1.4821095949709415\n",
      "Generated sequence: WN.subpliv'.re'blicayema'],\n",
      "    'INTEITWCT:)): .format_from import position\n",
      "\n",
      "if self.result = 0\n",
      "    f\n",
      "\n",
      "iter:55999/100000 loss:1.4919475827515125\n",
      "Generated sequence: WRS ARudule histing orriven\n",
      "winter./gopy''\n",
      "tet_page\n",
      "\"\"http % '()\n",
      "        numbers += orralidicl_module\n",
      "\n",
      "iter:56999/100000 loss:1.4598278650417924\n",
      "Generated sequence: Willher(\"Optioningtolle-5):\n",
      "    all_mile = ifgnuments[pode.comap2ation + 'cluage' = zos.Random_id))}\n",
      "\n",
      "\n",
      "iter:57999/100000 loss:1.4709833336621523\n",
      "Generated sequence: With \n",
      "        left += 1\n",
      "        max_value = 0.5:\n",
      "        if har2:\n",
      "        return True\n",
      "\n",
      "# Shanned from\n",
      "\n",
      "iter:58999/100000 loss:1.4670311392508446\n",
      "Generated sequence: Words\n",
      "from sklparits\n",
      "        if num2 == 0:\n",
      "                    devic = default_Q_returns(search_data_\n",
      "\n",
      "iter:59999/100000 loss:1.4581528226546943\n",
      "Generated sequence: We number\n",
      " print(list): \n",
      "        else:\n",
      " remove_db is ele the Stasting instr to Predictions a max sted\n",
      "\n",
      "iter:60999/100000 loss:1.4695404433459043\n",
      "Generated sequence: Window.ade_lof_get(s2condelle, max_text)\n",
      "        big=1)\n",
      "        print(list)\n",
      "\n",
      "\n",
      "def find_charset='moent\n",
      "\n",
      "iter:61999/100000 loss:1.472352461129427\n",
      "Generated sequence: Winnersing as parterStr\n",
      "import pandas as pd\n",
      "\n",
      "# link: Python,r_zile dataset, vector\n",
      "from nporm_id,\n",
      "   \n",
      "\n",
      "iter:62999/100000 loss:1.4706236670166255\n",
      "Generated sequence: W']}'\n",
      "    column = arg[3]: \n",
      "            search_ewe = Ems[] \n",
      "  def __init__(self,.name):\n",
      " env = name\n",
      " \n",
      "\n",
      "iter:63999/100000 loss:1.4858381550610065\n",
      "Generated sequence: Worn rysiG>THoun, featureState(self, name, character, stecs):\n",
      " merged = dp.lamb(txYKe (k) # 1010, i -\n",
      "\n",
      "iter:64999/100000 loss:1.448519297592342\n",
      "Generated sequence: WAlien\n",
      "bot = \" intlinesizinged iust 2 i achewOb3\" B {0}'\n",
      "\n",
      "# Create the strings of \"This \n",
      " ### store s\n",
      "\n",
      "iter:65999/100000 loss:1.472979130603373\n",
      "Generated sequence: WiMLv_one(char] \n",
      "        #print the nums\n",
      "import random\n",
      "\n",
      "def display_id = 'home feature has accuracyIr\n",
      "\n",
      "iter:66999/100000 loss:1.437590517666191\n",
      "Generated sequence: Word3065604ezed\", random.choice(lst))\n",
      "    \n",
      "\n",
      "classifier_even(arr, n): \n",
      "    if frames in rate.texts.pat\n",
      "\n",
      "iter:67999/100000 loss:1.472152998842299\n",
      "Generated sequence: Wincbouce ac to fieldswords targees node vikite -d to Pathor\n",
      "    sqlitic_time = max(to_faper() witt s\n",
      "\n",
      "iter:68999/100000 loss:1.4349516808763148\n",
      "Generated sequence: W', icon_rates(X_b, -Bens_message, x), wind, attr.bondliver.names)\n",
      "#\n",
      "    ASu title has placebon-toria\n",
      "\n",
      "iter:69999/100000 loss:1.4488962046504021\n",
      "Generated sequence: We code feature a board\n",
      "        words_list = random.randevation(\"\\n\").fet('There the is\"\n",
      "\n",
      "\n",
      "def reques\n",
      "\n",
      "iter:70999/100000 loss:1.453378909945488\n",
      "Generated sequence: Wessap(olds=10, element) = a**2*y\n",
      "                r = j < 3 import armat\n",
      "from natityrien.cost_url\n",
      "\n",
      "  \n",
      "\n",
      "iter:71999/100000 loss:1.4371164753437042\n",
      "Generated sequence: Width\n",
      "\n",
      " and('templink_ins=_face.matn', how_input_number):\n",
      "    target} + 'student'Sum1.chars.statil()\n",
      "\n",
      "\n",
      "iter:72999/100000 loss:1.4310790752097964\n",
      "Generated sequence: Windowers)\n",
      "\n",
      "# Define the modtName\n",
      "def set_ts(args, vli):\n",
      "  \n",
      "        imagest = [24, 301, 12\n",
      "    return\n",
      "\n",
      "iter:73999/100000 loss:1.4477304050028323\n",
      "Generated sequence: Width=quitXExendstinn.get(ame, s2, r)\n",
      "        char_prime_numing = nel.find(cls))\n",
      "\n",
      "\n",
      "import random\n",
      "dutu\n",
      "\n",
      "iter:74999/100000 loss:1.459045935869217\n",
      "Generated sequence: Wn_np.array(['Generate Set['Googramm)]','batablemore):''\" \n",
      "            image = sub[i], twotfserDing_b\n",
      "\n",
      "iter:75999/100000 loss:1.4579575064182282\n",
      "Generated sequence: WILIR :\n",
      "#The definanone Gtk text for a params whileLoat see the basitali-10009 are 4.0.5.mb)\n",
      "    toke\n",
      "\n",
      "iter:76999/100000 loss:1.4490398806035518\n",
      "Generated sequence: Weachorirsy and episuable of the text manage in detee any as net prepact if key c: ''\n",
      "\n",
      "    # convertl\n",
      "\n",
      "iter:77999/100000 loss:1.4583172377385198\n",
      "Generated sequence: WIENTERIETDL_RCVALLTOD = self.marcc[index][9] == self.urlibnames.RowB.gdome'))\n",
      "        migr] = ip[0 ]\n",
      "\n",
      "iter:78999/100000 loss:1.418991726487875\n",
      "Generated sequence: Wrultiul modelestasts[char.ROMNMENDEWETIFITY])\n",
      "        maxList = [i in __init__(self, self):\n",
      "        \n",
      "\n",
      "iter:79999/100000 loss:1.4441676042377949\n",
      "Generated sequence: With file\n",
      "bubble = ;\n",
      "    \n",
      "                    if arr[j], k\n",
      "  print(125,\n",
      "         responseArookercase \n",
      "\n",
      "iter:80999/100000 loss:1.4158566925525666\n",
      "Generated sequence: WNLECTE.rYz_gg enument\")\n",
      "    result = 0\n",
      "  \n",
      "  return result.req(self.array) \n",
      "    return data[str2[0]\n",
      "f\n",
      "\n",
      "iter:81999/100000 loss:1.435491056162864\n",
      "Generated sequence: Whatabload(+ bheaded - [i+1, len(arr)):                         db.sg\"\n",
      " foreds: \n",
      "        print('--= (\n",
      "\n",
      "iter:82999/100000 loss:1.460163809031248\n",
      "Generated sequence: Wordent\n",
      "        return number. * x</1:\n",
      "            return i)\n",
      "            if elements == False: \n",
      "    s\n",
      "\n",
      "iter:83999/100000 loss:1.450179513990879\n",
      "Generated sequence: Wyfiplf.write_box,1,2)\n",
      "  for euderatf in databars_has_verial_thx_list:\n",
      "    x = len(greage, ))\n",
      "       \n",
      "\n",
      "iter:84999/100000 loss:1.406780102994293\n",
      "Generated sequence: Ward = screen_html.print(self)]\n",
      "    print(words)\n",
      "\n",
      "\n",
      "    # Boot the sum\n",
      "        sorted_heapErrrow_shift\n",
      "\n",
      "iter:85999/100000 loss:1.432448805047199\n",
      "Generated sequence: Wes = {}\n",
      "\n",
      "    for num in char_list:\n",
      " len(arr1) - 1, j+1:\n",
      "        if fast an    i != 0:\n",
      "              \n",
      "\n",
      "iter:86999/100000 loss:1.427405740045011\n",
      "Generated sequence: WABATAUFTERTROR, None = filter_id=(31:):\n",
      " total = FlagVDLEST * FROMs_Search(aifume_name=\"GBZ\")\n",
      "      \n",
      "\n",
      "iter:87999/100000 loss:1.4221121095642448\n",
      "Generated sequence: Wibondings:\n",
      "  \n",
      "def __init__(self, f):\n",
      " debuge_name\n",
      "    y_words = dataSubx+\n",
      "            dictivation. C\n",
      "\n",
      "iter:88999/100000 loss:1.4476820606291294\n",
      "Generated sequence: World_asswer\n",
      "\"Params:%s\".get(.', title),\n",
      "                                                            \n",
      "\n",
      "iter:89999/100000 loss:1.4492789807617665\n",
      "Generated sequence: WOY\n",
      "    redi_seon(string)\n",
      "        self.features.fooes(\" * kerash by or the chringting each not sprite\n",
      "\n",
      "iter:90999/100000 loss:1.4265263224542142\n",
      "Generated sequence: Words.Wergere.compote_mhoited\n",
      "\n",
      "mos_tus = [4,9,4,5,8,55], random_list)\n",
      "\n",
      "# Create two account the load \n",
      "\n",
      "iter:91999/100000 loss:1.3812511901557445\n",
      "Generated sequence: Warnelse:\n",
      " upllab.qv_shopuange('Carter is list1'\n",
      " while (s0)\n",
      "\n",
      "\n",
      "class GewEY SK \"He>b_pro1:\n",
      "    test_nu\n",
      "\n",
      "iter:92999/100000 loss:1.4297440538629889\n",
      "Generated sequence: WPATE __initialize(key=0): ordvicework datasets\n",
      "from sklearn.model_specbers == n\" append3.cape\n",
      "\n",
      "reque\n",
      "\n",
      "iter:93999/100000 loss:1.4399445389769971\n",
      "Generated sequence: Word2}IJEVe ind an x  \n",
      " val_ind_string = lect) + '.yutf-8')\n",
      "\n",
      "\n",
      "from staretion/phranes\"\n",
      "\t\t\tif toweb_app\n",
      "\n",
      "iter:94999/100000 loss:1.4164394582808018\n",
      "Generated sequence: WUkpendate and label:\n",
      "                parser.dolect((img.used, request()\n",
      "\n",
      "def get_seck(bs = [glas.qua\n",
      "\n",
      "iter:95999/100000 loss:1.433636426538229\n",
      "Generated sequence: WHITE_HEMO_BS_DOGIOR == '':\n",
      "    # Given Olliem bubble is and tx new\n",
      "    A = None\n",
      "\n",
      "\n",
      "def accuracy(strin\n",
      "\n",
      "iter:96999/100000 loss:1.3724347394965588\n",
      "Generated sequence: WY-J0):\n",
      "            for j in range(2,V):\n",
      "            rine = 1\n",
      "        defaultField(order):\n",
      "    total \n",
      "\n",
      "iter:97999/100000 loss:1.425779143512249\n",
      "Generated sequence: Words in root of mitient:\\tTwentort Calir paimg',\n",
      "                'sorted': 134T0990,, \"Id>\",\n",
      "       \n",
      "\n",
      "iter:98999/100000 loss:1.4173960150182248\n",
      "Generated sequence: W\"\n",
      "    return threaher()\n",
      "\n",
      "\n",
      "def from_words(token, Greateration, fibonacci(n):\n",
      "    return False\n",
      "    pri\n",
      "\n",
      "iter:99999/100000 loss:1.4288028580211103\n",
      "Generated sequence: Will b                 rc.LabelWord \"Stats\"\n",
      "\n",
      "    k, i]\n",
      "        # check if max Dos is distance in sets\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "hidden_size = 128\n",
    "net = MyLSTM(n_chars, hidden_size, n_chars).to(device)\n",
    "opt = optim.Adam(net.parameters(), lr=0.003)\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=-1)  # Ignore the special index -1\n",
    "\n",
    "iters = 100000\n",
    "print_iters = 1000\n",
    "loss_sum = 0\n",
    "all_losses = []\n",
    "\n",
    "for i in range(iters):\n",
    "    input, target = get_input_and_target(batch_size)\n",
    "    input, target = input.to(device), target.to(device)\n",
    "    loss = train_step(net, opt, input, target)\n",
    "    loss_sum += loss\n",
    "\n",
    "    if i % print_iters == print_iters - 1:\n",
    "        print('iter:{}/{} loss:{}'.format(i, iters, loss_sum / print_iters))\n",
    "        print('Generated sequence: {}\\n'.format(eval_step(net)))\n",
    "        all_losses.append(loss_sum / print_iters)\n",
    "        loss_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('iters')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm is 1 data array number:\n",
      " \n",
      "    return pold.read \n",
      " \trank\n",
      "\n",
      "# Calculate be frequent in the bests\n",
      "  return employee(s2, self):\n",
      "        if '%HD,' variant' to data input == direa.value:\n",
      "        else:\n",
      "    else:\n",
      "        pred = rngenear.longte.noce.funLEDICERD())\n",
      "  def remove_documes():\n",
      " perfoomd_fingRum2, has.words('Cad, \"* {(url)]\n",
      "import numpy as np\n",
      "fursing=models.user(ample, value=)\n",
      "X_tit = TI(s1[2].string_tokene)\n",
      "        for validate is paramilies\n",
      "\n",
      "# \n",
      "    return True\n",
      "    return model.randomFrawe[pripeque,\n",
      " rad_ATIM)\n",
      "\n",
      "# Tak kense\n",
      "        cur = addres.spam([\"meta's, current, state-289r)@model.stat\n"
     ]
    }
   ],
   "source": [
    "print(eval_step(net, predicted_len=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WM3Greisive new_books.print_path(s,\"s)])\n",
      "\n",
      "    # Drecinalize to second models path(X], \"\"\n",
      "if ((4))\n",
      "    else:\n",
      "\tsubert_wait(copy)\n",
      " \n",
      "        n = 10\n",
      "        min = len(input))\n",
      "        request=50\n",
      "        for exts if end proce:\n",
      "    perifich.'x.stetle'\n",
      "  # Initialize in lack \n",
      "    print buscom\n",
      "\n",
      "\n",
      "def add(Struce\n"
     ]
    }
   ],
   "source": [
    "print(eval_step(net, predicted_len=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will rturationIterator\n",
      "end = [1, 3, 5, 6 ]\n",
      "\n",
      "\n",
      "def text((len(this,)):\n",
      "        funcVar = 20\n",
      "\n",
      "# This ald tag is frequencied node\n",
      "lette = 5]\n",
      "def get(\\n, longest_sum):\n",
      "  min(while  x):\n",
      "            return self, s2) \n",
      "    for char in nums:\n",
      "    dict[i] = (n] \n",
      "\n",
      "    # fost the gradion auther if the predictions\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(eval_step(net, predicted_len=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTMWrapper(MyLSTM):\n",
    "    def forward(self, input):\n",
    "        batch_size = input.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        output, _ = super(MyLSTMWrapper, self).forward(input, hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = 100 \n",
    "hidden_size = 128 \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLSTMWrapper(n_chars, hidden_size, n_chars).to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
